{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"PictoPy","text":"<p>PictoPy is a modern desktop app designed to transform the handling of digital photos. It facilitates efficient gallery management with a robust focus on privacy, offering smart tagging capabilities for photos based on objects, faces, or scenes.</p> <p></p> <p>This project was announced by AOSSIE, an umbrella organization and was to be implemented from scratch. It provides features such as object detection and face similarity, offering smart tagging capabilities for photos based on objects, faces.</p> Overview <ul> <li>                    Features                   </li> <li>                    Architecture                   </li> </ul> Backend Python <ul> <li>                      Database                  </li> <li>                      Directory Structure                  </li> <li>                      API                  </li> <li>                      Image Processing                  </li> </ul> Backend Rust <ul> <li>                      API                  </li> </ul> Frontend <ul> <li>                      UI Components                  </li> <li>                      State Management                  </li> <li>                      Gallery View                  </li> </ul>"},{"location":"Manual_Setup_Guide/","title":"Manual Setup Guide","text":""},{"location":"Manual_Setup_Guide/#manual-setup-guide","title":"Manual Setup Guide","text":""},{"location":"Manual_Setup_Guide/#initial-steps","title":"Initial Steps:","text":""},{"location":"Manual_Setup_Guide/#1-fork-the-pictopy-repository-httpsgithubcomaossie-orgpictopy","title":"1. Fork the PictoPy repository: https://github.com/AOSSIE-Org/PictoPy","text":""},{"location":"Manual_Setup_Guide/#2-open-your-terminal-linuxmacos-or-powershell-windows","title":"2. Open your Terminal (Linux/MacOS) or Powershell (Windows)","text":""},{"location":"Manual_Setup_Guide/#3-clone-your-forked-repository","title":"3. Clone your forked repository:","text":"<pre><code>git clone https://github.com/yourUsername/PictoPy\n</code></pre>"},{"location":"Manual_Setup_Guide/#4-change-to-the-repository-directory","title":"4. Change to the repository directory:","text":"<pre><code>cd PictoPy\n</code></pre>"},{"location":"Manual_Setup_Guide/#5-add-the-main-repository-as-upstream","title":"5. Add the main repository as \"upstream\":","text":"<pre><code>git remote add upstream https://github.com/AOSSIE-Org/PictoPy\n</code></pre>"},{"location":"Manual_Setup_Guide/#tauri-frontend-setup","title":"Tauri Frontend Setup:","text":"<ol> <li> <p>Install Tauri prerequisites based on your OS using this guide.</p> </li> <li> <p>Navigate to the Frontend Directory: Open your terminal and use <code>cd</code> to change directories:    <pre><code>cd frontend\n</code></pre></p> </li> <li>Install Dependencies:    <pre><code>npm install\n</code></pre></li> <li>Start the Tauri desktop app in development mode <pre><code>npm run tauri dev\n</code></pre></li> </ol>"},{"location":"Manual_Setup_Guide/#python-fastapi-backend-setup-steps","title":"Python (FastAPI) Backend Setup Steps:","text":"<p>Note: For backend setup make sure that you have Python version 3.12. Additionally, for Windows, make sure that you are using Powershell for the setup, not command prompt.</p> <ol> <li> <p>Navigate to the Backend Directory: Open your terminal and use <code>cd</code> to change directories:</p> <p>Bash</p> <pre><code>cd backend\n</code></pre> </li> <li> <p>Set Up a Virtual Environment (Highly Recommended): Virtual environments isolate project dependencies. Create one using:</p> <p>Bash(Linux/MacOS)</p> <pre><code>python3 -m venv .env\n</code></pre> <p>Powershell(Windows)</p> <pre><code>python -m venv .env\n</code></pre> </li> <li> <p>Activate the Virtual Environment:</p> <p>Bash(Linux/MacOS)</p> <pre><code>source .env/bin/activate\n</code></pre> <p>Powershell(Windows)</p> <pre><code>.env\\Scripts\\activate.ps1\n</code></pre> <p>After activating, you should be able to see the virtual environment's name before the current path. Something like this:</p> <p></p> </li> <li> <p>Install Dependencies: The <code>requirements.txt</code> file lists required packages. Install them using pip:</p> <p>Bash</p> <pre><code>pip install -r requirements.txt\n</code></pre> </li> <li> <p>Running the backend:: To start the backend in development mode, run this command while being in the backend folder and the virtual environment activated:</p> <p>Bash/Powershell</p> <pre><code>fastapi dev\n</code></pre> <p>The server will start on <code>http://localhost:8000</code> by default. In test mode, the server will automatically restart if any errors are detected or if source files are modified.</p> <p></p> </li> <li> <p>Missing System Dependencies: Some dependencies might need system-level libraries like <code>libGL.so.1</code> (often needed by OpenCV). Install the appropriate packages based on your distribution:</p> <p>Debian/Ubuntu:</p> <p>Bash</p> <pre><code>sudo apt update\nsudo apt install -y libglib2.0-dev libgl1-mesa-glx\n</code></pre> <p>Other Systems: Consult your distribution's documentation for installation instructions.</p> </li> <li> <p><code>gobject-2.0</code> Not Found Error: Resolve this error by installing <code>libglib2.0-dev</code> (Debian/Ubuntu):</p> <p>Bash</p> <pre><code>sudo apt install -y libglib2.0-dev pkg-config\n</code></pre> <p>For other systems, consult your distribution's documentation.</p> </li> </ol>"},{"location":"Script_Setup_Guide/","title":"Script Setup Guide","text":""},{"location":"Script_Setup_Guide/#setting-up-using-script","title":"Setting Up using Script","text":""},{"location":"Script_Setup_Guide/#video-setup-guide","title":"Video Setup Guide:","text":"<ul> <li>Windows</li> <li>Ubuntu (Debian)</li> </ul>"},{"location":"Script_Setup_Guide/#prerequisites","title":"Prerequisites:","text":"<ul> <li>NodeJS (LTS Version Recommended)</li> <li>Git version control system</li> </ul>"},{"location":"Script_Setup_Guide/#steps-performed-in-the-video","title":"Steps Performed in the Video:","text":"<ol> <li> <p>Fork the PictoPy repository: https://github.com/AOSSIE-Org/PictoPy</p> </li> <li> <p>Open your terminal (or Powershell with administrator privileges on Windows)</p> </li> <li> <p>Clone your forked repository:</p> </li> </ol> <pre><code>git clone https://github.com/yourUsername/PictoPy\n</code></pre> <ol> <li>Change to the repository directory:</li> </ol> <pre><code>cd PictoPy\n</code></pre> <ol> <li>Add the main repository as \"upstream\":</li> </ol> <pre><code>git remote add upstream https://github.com/AOSSIE-Org/PictoPy\n</code></pre> <ol> <li>Run the Automatic Setup</li> </ol> <pre><code>npm run setup\n</code></pre> <p>Note: This step can take a long time depending on your internet connection and system specifications. If the script seems to stop progressing after waiting for more than 10 minutes, press Enter in your terminal window to continue.</p> <ol> <li>Start the Backend Server</li> </ol> <p>#### Windows</p> <pre><code>cd .\\backend\n.env\\Scripts\\activate.ps1\nfastapi dev\n</code></pre> <p>#### Linux</p> <pre><code>cd ./backend\nsource .env/bin/activate\nfastapi dev\n</code></pre> <ol> <li>Start the Frontend Desktop App</li> </ol> <p>Open a new terminal window, navigate to the project directory, and run:</p> <pre><code>cd frontend\nnpm run tauri dev\n</code></pre> <ol> <li>Pre-commit Setup</li> </ol> <p>Before running the <code>git commit</code> command, ensure you have the following Python packages installed globally:</p> <pre><code>pip install ruff black mypy pre-commit\n</code></pre> <p>Note: If you are committing from a virtual environment, these packages should already be installed as they are included in the requirements.txt file.</p>"},{"location":"setup/","title":"Setup Instructions","text":"<p>Please refer to the CONTRIBUTING.md file for complete setup instructions.</p>"},{"location":"setup/#available-setup-methods","title":"Available Setup Methods","text":"<p>The CONTRIBUTING.md file contains detailed instructions for:</p> <ol> <li>Script Setup - Automated setup using our setup script, recommended only for Windows and Debian-based OS like Ubuntu.</li> <li>Manual Setup - Step-by-step manual installation, recommended for other operating systems.</li> <li>Documentation Website Setup Guide</li> </ol>"},{"location":"setup/#quick-links","title":"Quick Links","text":"<ul> <li>Main Repository</li> <li>Setup Section in CONTRIBUTING.md</li> <li>Troubleshooting</li> </ul>"},{"location":"backend/backend_python/api/","title":"Python Backend API","text":""},{"location":"backend/backend_python/database/","title":"Database","text":""},{"location":"backend/backend_python/database/#overview","title":"Overview","text":"<p>PictoPy uses several SQLite databases to manage various aspects of the application. This document provides an overview of each database, its structure, and its primary operations.</p>"},{"location":"backend/backend_python/database/#database-schema","title":"Database Schema","text":"<p>You can view the interactive DB schema below:  </p> <p></p> <p>Alternatively, click here to open the DB diagram in a new tab.</p>"},{"location":"backend/backend_python/directory-structure/","title":"Directory Structure","text":"<p>The entry point for the backend is in <code>main.py</code>, which initializes the databases and handles the startup and shutdown for the FastAPI server.</p> <p>The code for the application mainly lies in the <code>app/</code> directory the heirarchy of which looks like this:</p> <pre><code>.\n\u251c\u2500\u2500 main.py\n\u2514\u2500\u2500 app/\n    \u251c\u2500\u2500 config/\n    \u251c\u2500\u2500 database/\n    \u251c\u2500\u2500 models/\n    \u251c\u2500\u2500 routes/\n    \u251c\u2500\u2500 schemas/\n    \u2514\u2500\u2500 utils/\n</code></pre> <p>We will discuss what each of these directories do and the relevant files they contain</p>"},{"location":"backend/backend_python/directory-structure/#config","title":"config","text":"<p>Related to variables used accross the application.</p> Name Description <code>settings.py</code> Contains configuration files for the application, mainly paths and parameters which are used across the application"},{"location":"backend/backend_python/directory-structure/#database","title":"database","text":"<p>This directory contains files related to database operations, including table creation, query handeling and some helper functions on the tables. These files are the places where most of the SQL queries are written. By default, on startup this directory is where the databases (<code>.db</code> files) is created.</p> Name Description <code>albums.py</code> Handles operations related to photo albums, including creating, deleting, and managing albums and their contents. <code>face_clusters.py</code> Provides functions to create, insert, update, retrieve, and delete face cluster records along with related images. <code>faces.py</code> Manages face-related data, including storing and retrieving face embeddings for facial recognition. <code>folders.py</code> Handles operations to create, insert, update, retrieve, and delete folder records, while handling folder hierarchies and AI tagging status. <code>images.py</code> Deals with image-related operations, such as storing image metadata, managing image IDs, and handling image classifications. <code>metadata.py</code> Manages the metadata and provides functions to create the table, retrieve stored metadata as a dictionary, and update the metadata with new values. <code>yolo_mapping.py</code> Creates and manages mappings for YOLO object detection classes."},{"location":"backend/backend_python/directory-structure/#models","title":"models","text":"<p>This directory contains pre-trained machine learning models used in the application.</p> Name Description <code>FaceDetector.py</code> a FaceDetector class for detecting faces in an image <code>FaceNet.py</code> Pre-trained FaceNet model for generating face embeddings <code>ObjectClassifier.py</code> Detects objects in images and returns their class IDs <code>YOLO.py</code> YOLO ONNX detects objects, outputs boxes."},{"location":"backend/backend_python/directory-structure/#routes","title":"routes","text":"<p>This directory contains API route definitions for different functionalities of the application.</p> Name Description <code>albums.py</code> Handles API routes for album-related operations (create, delete, add/remove photos, view albums) <code>face_clusters.py</code> Rename clusters, list clusters, and fetch cluster images. <code>facetagging.py</code> Manages routes for face matching, clustering, and finding related images <code>folders.py</code> Add, sync, update AI tagging, delete, and list folders, managing folder hierarchy and image processing asynchronously. <code>images.py</code> Deals with image-related operations (adding, deleting, retrieving images and their metadata) <code>user_preferences.py</code> Get and update user preferences stored in the metadata database."},{"location":"backend/backend_python/directory-structure/#schemas","title":"schemas","text":"<p>This directory contains Pydantic models defining the structure and validation of data exchanged through the API endpoints.</p> Name Description <code>album.py</code> For validating and structuring album-related API requests. <code>face_clusters.py</code> For requests and responses related to face cluster management. <code>facetagging.py</code> Face matching, clustering, related images, and error responses. <code>folders.py</code> Folder-related API requests, responses, and data structures <code>images.py</code> Image management requests and responses, including deletions. <code>test.py</code> Tests image detection requests, responses, and error handling. <code>user_preferences.py</code> User preferences API requests, responses, and error handling."},{"location":"backend/backend_python/directory-structure/#utils","title":"utils","text":"<p>This directory contains utility functions and helper modules used across the application.</p> Name Description <code>API.py</code> Sends POST request to restart sync microservice, logs success or failure <code>face_clusters.py</code> Clusters face embeddings, updates clusters, generates cluster images. <code>FaceNet.py</code> Preprocesses images, normalizes embeddings, computes similarity. <code>folders.py</code> Manages folder trees: add, delete, sync folders in database and filesystem. <code>image_metadata.py</code> Extracts image metadata including EXIF,size,format, and creation date safely <code>images.py</code> Processes images in folders: thumbnails, detects faces, classifies,updates DB <code>memory_monitor.py</code> Decorator logs memory usage and execution time of functions. <code>microservice.py</code> Starts sync microservice with virtual environment or bundled executable. <code>ONNX.py</code> Returns ONNX execution providers list based on GPU acceleration preference. <code>YOLO.py</code> YOLO utilities for NMS, drawing, and model path from preferences."},{"location":"backend/backend_python/image-processing/","title":"Image Processing","text":"<p>We use Python\u2019s Process Pool Executor for parallel image processing in background worker processes. This allows multiple images to be processed in parallel without blocking the API or frontend.</p> <p>PictoPy uses different models for achieving its tagging capabilities. The discussed models below are default models, you can change them by going to <code>app/models</code> directory and change the paths in the configuration files.</p>"},{"location":"backend/backend_python/image-processing/#object-detection-with-yolov11","title":"Object Detection with YOLOv11","text":"<p>We use YOLOv11 to spot objects in your photos. Here's what it does:</p> <p>YOLOv11 takes your image and runs it through its model. It figures out what objects are in the image and where they are. The result is a list of objects, their locations, and how confident the model is about each detection. If a <code>person</code> class is predicted we pass it on to the face detection model which we discuss in the next section.</p> Fun Fact <p>YOLO stands for \"You Only Look Once\". We use the model provided by Ultralytics by default.</p>"},{"location":"backend/backend_python/image-processing/#face-detection-and-recognition","title":"Face Detection and Recognition","text":"<p>For faces, we do a bit more:</p> <p>We start with a special version of YOLOv11 that's really good at finding faces. Once we find a face, we zoom in on it (by cropping it to <code>160x160</code> - the shape FaceNet expects) and pass it to our FaceNet model. FaceNet then creates a unique 'embedding' for each face, the representation of the face in a form of numbers.</p> Fun Fact <p>We use another YOLOv11 model for this as well by default. This was pretrained on top of the one provided by Ultralytics and is called yolov11-face</p> What's an embedding? <p>An embedding is a bunch of numbers that represent the face. Similar faces will have similar numbers. FaceNet creates a 512-dimensional embedding array for each detected face in the image.</p>"},{"location":"backend/backend_python/image-processing/#face-clustering","title":"Face Clustering","text":"<p>Now, here's where it gets interesting:</p> <p>We use something called DBSCAN to group similar faces together. This process happens automatically as you add new photos to the system, we perform reclustering after every 5 photos are added (this can be changed in the code) but apart from that, the photos are assigned a cluster based on the embedding distance of the faces in the photo with the mean of each of the clusters.</p>"},{"location":"backend/backend_python/image-processing/#how-it-all-fits-together","title":"How It All Fits Together","text":"<p>When you add a new photo, we first look for objects and faces. If we find faces, we generate embeddings for them. These embeddings then get added to our face clusters. All this information gets stored in our database so we can find it later.</p>"},{"location":"backend/backend_python/image-processing/#under-the-hood","title":"Under the Hood","text":"<p>We're using ONNX runtime to run our AI models quickly. Everything's stored in SQLite databases, making it easy to manage. The system updates clusters as you add or remove photos, so it keeps getting smarter over time.</p>"},{"location":"backend/backend_python/image-processing/#pictopy-model-parameters","title":"PictoPy Model Parameters","text":"<p>Here are some key parameters for the main models used in PictoPy's image processing pipeline.</p>"},{"location":"backend/backend_python/image-processing/#yolov11-object-detection","title":"YOLOv11 Object Detection","text":"Parameter Value Description <code>conf_thres</code> 0.4 Confidence threshold for object detection <code>iou_thres</code> 0.5 IoU (Intersection over Union) threshold for NMS Input Shape Varies Determined dynamically from the model Output Multiple Includes bounding boxes, scores, and class IDs"},{"location":"backend/backend_python/image-processing/#face-detection-yolov11-variant","title":"Face Detection (YOLOv11 variant)","text":"Parameter Value Description <code>conf_thres</code> 0.35 Confidence threshold for face detection <code>iou_thres</code> 0.45 IoU threshold for NMS in face detection Model Path <code>DEFAULT_FACE_DETECTION_MODEL</code> Path to the face detection model file"},{"location":"backend/backend_python/image-processing/#facenet-face-recognition","title":"FaceNet (Face Recognition)","text":"Parameter Value Description Model Path <code>DEFAULT_FACENET_MODEL</code> Path to the FaceNet model file Input Shape (1, 3, 160, 160) Expected input shape for face images Output 512-dimensional vector Face embedding dimension"},{"location":"backend/backend_python/image-processing/#face-clustering-dbscan","title":"Face Clustering (DBSCAN)","text":"Parameter Value Description <code>eps</code> 0.3 Maximum distance between two samples for them to be considered as in the same neighborhood <code>min_samples</code> 2 Number of samples in a neighborhood for a point to be considered as a core point <code>metric</code> \"cosine\" Distance metric used for clustering <p>Note: Some of these values are default parameters and can be adjusted when initializing the models or during runtime, depending on the specific use case or performance requirements.</p>"},{"location":"backend/backend_rust/api/","title":"Rust Backend API","text":""},{"location":"backend/backend_rust/api/#api-documentation","title":"API Documentation","text":"<p>The Rust backend provides the following command that can be invoked from the frontend:</p>"},{"location":"backend/backend_rust/api/#get_server_path","title":"get_server_path","text":"<ul> <li>Description: Retrieves the path to the server resources directory.</li> <li>Parameters: None</li> <li>Returns: <code>Result&lt;String, String&gt;</code></li> </ul>"},{"location":"backend/backend_rust/api/#usage-examples","title":"Usage Examples","text":"<pre><code>// In your frontend JavaScript/TypeScript code:\nimport { invoke } from \"@tauri-apps/api/tauri\";\n\n// Example: Get server path\nconst serverPath = await invoke(\"get_server_path\");\nconsole.log(\"Server path:\", serverPath);\n</code></pre>"},{"location":"backend/backend_rust/api/#cross-platform-support","title":"Cross-Platform Support","text":"<p>The API provides cross-platform support using Tauri's unified <code>AppHandle.path().resolve(..., BaseDirectory::Resource)</code> for path resolution across Windows, macOS, and Linux.</p> <p>This backend provides essential path resolution functionality for the Tauri application.</p>"},{"location":"frontend/screenshots/","title":"Screenshots","text":"<p>This section showcases the PictoPy application interface with sample photos featuring people. The screenshots demonstrate the various features and user interface components of the application.</p>"},{"location":"frontend/screenshots/#main-gallery-view","title":"Main Gallery View","text":"<p>The main gallery displays photos in a grid layout, organized by date with filtering options.</p> <p> Main gallery view showing a collection of photos with people, organized in a responsive grid layout</p>"},{"location":"frontend/screenshots/#ai-tagging-features","title":"AI Tagging Features","text":"<p>The application includes AI-powered features for intelligent photo organization, including automatic object detection, face recognition, and smart clustering.</p> <p> AI Tagging interface showing face collections, object detection, and intelligent photo organization</p>"},{"location":"frontend/screenshots/#settings-panel","title":"Settings Panel","text":"<p>The settings panel allows users to configure directories, preferences, and application behavior.</p> <p> Settings panel showing directory configuration and user preferences</p>"},{"location":"frontend/state-management/","title":"State Management with Redux","text":"<p>This guide outlines the Redux-based state management system used in our PictoPy application, focusing on Redux slices and store configuration.</p>"},{"location":"frontend/state-management/#overview","title":"Overview","text":"<p>Our application uses Redux Toolkit for state management, which provides:</p> <ul> <li>Redux slices for feature-based state organization</li> <li>Immutable state updates with Immer</li> <li>TypeScript integration for type safety</li> </ul> <p>The Redux store serves as the single source of truth for application state that needs to be shared across multiple components.</p>"},{"location":"frontend/state-management/#store-structure","title":"Store Structure","text":"<p>Our Redux store is organized into the following slices:</p>"},{"location":"frontend/state-management/#1-images-slice","title":"1. Images Slice","text":"<p>Manages the state for images and media viewing operations.</p> <p>State Structure:</p> <pre><code>interface ImageState {\n  images: Image[];\n  currentViewIndex: number;\n  totalImages: number;\n  error: string | null;\n}\n</code></pre> <p>Key Actions:</p> <ul> <li><code>setImages</code> - Updates the images array</li> <li><code>addImages</code> - Adds new images to the array</li> <li><code>setCurrentViewIndex</code> - Sets the currently viewed image index</li> <li><code>nextImage</code> - Navigates to the next image</li> <li><code>previousImage</code> - Navigates to the previous image</li> <li><code>closeImageView</code> - Closes the image viewer</li> <li><code>updateImage</code> - Updates specific image data</li> <li><code>removeImage</code> - Removes an image from the array</li> <li><code>setError</code> - Sets error state</li> <li><code>clearImages</code> - Clears all image data</li> </ul>"},{"location":"frontend/state-management/#2-folders-slice","title":"2. Folders Slice","text":"<p>Manages folder-related state and operations.</p> <p>State Structure:</p> <pre><code>interface FolderState {\n  folders: FolderDetails[];\n}\n</code></pre> <p>Key Actions:</p> <ul> <li><code>setFolders</code> - Updates the folders array</li> <li><code>addFolder</code> - Adds a new folder or updates existing one</li> <li><code>updateFolder</code> - Modifies an existing folder</li> <li><code>removeFolders</code> - Removes folders by IDs</li> <li><code>clearFolders</code> - Clears all folder data</li> </ul>"},{"location":"frontend/state-management/#3-face-clusters-slice","title":"3. Face Clusters Slice","text":"<p>Handles face recognition clusters and naming.</p> <p>State Structure:</p> <pre><code>interface FaceClustersState {\n  clusters: Cluster[];\n}\n</code></pre> <p>Key Actions:</p> <ul> <li><code>setClusters</code> - Updates the clusters array</li> <li><code>updateClusterName</code> - Updates a cluster's name</li> </ul>"},{"location":"frontend/state-management/#4-onboarding-slice","title":"4. Onboarding Slice","text":"<p>Manages the user onboarding process and user profile.</p> <p>State Structure:</p> <pre><code>interface OnboardingState {\n  currentStepIndex: number;\n  currentStepName: string;\n  stepStatus: boolean[];\n  avatar: string | null;\n  name: string;\n}\n</code></pre> <p>Key Actions:</p> <ul> <li><code>setAvatar</code> - Sets user avatar</li> <li><code>setName</code> - Sets user name</li> <li><code>markCompleted</code> - Marks an onboarding step as completed</li> <li><code>previousStep</code> - Goes back to the previous onboarding step</li> </ul>"},{"location":"frontend/state-management/#5-loader-slice","title":"5. Loader Slice","text":"<p>Manages loading states across the application.</p> <p>State Structure:</p> <pre><code>interface LoaderState {\n  loading: boolean;\n  message: string;\n}\n</code></pre> <p>Key Actions:</p> <ul> <li><code>showLoader</code> - Shows loading state with message</li> <li><code>hideLoader</code> - Hides loading state</li> </ul>"},{"location":"frontend/state-management/#6-info-dialog-slice","title":"6. Info Dialog Slice","text":"<p>Manages information dialog display and content.</p> <p>State Structure:</p> <pre><code>interface InfoDialogProps {\n  isOpen: boolean;\n  title: string;\n  message: string;\n  variant: InfoDialogVariant;\n  showCloseButton: boolean;\n}\n</code></pre> <p>Key Actions:</p> <ul> <li><code>showInfoDialog</code> - Shows information dialog with content</li> <li><code>hideInfoDialog</code> - Hides information dialog</li> </ul>"},{"location":"frontend/state-management/#redux-toolkit-configuration","title":"Redux Toolkit Configuration","text":""},{"location":"frontend/state-management/#store-setup","title":"Store Setup","text":"<pre><code>import { configureStore } from \"@reduxjs/toolkit\";\nimport loaderReducer from \"@/features/loaderSlice\";\nimport onboardingReducer from \"@/features/onboardingSlice\";\nimport imageReducer from \"@/features/imageSlice\";\nimport faceClustersReducer from \"@/features/faceClustersSlice\";\nimport infoDialogReducer from \"@/features/infoDialogSlice\";\nimport folderReducer from \"@/features/folderSlice\";\n\nexport const store = configureStore({\n  reducer: {\n    loader: loaderReducer,\n    onboarding: onboardingReducer,\n    images: imageReducer,\n    faceClusters: faceClustersReducer,\n    infoDialog: infoDialogReducer,\n    folders: folderReducer,\n  },\n});\n\nexport type RootState = ReturnType&lt;typeof store.getState&gt;;\nexport type AppDispatch = typeof store.dispatch;\n</code></pre>"},{"location":"frontend/state-management/#usage-in-components","title":"Usage in Components","text":""},{"location":"frontend/state-management/#connecting-components","title":"Connecting Components","text":"<p>Use the <code>useSelector</code> and <code>useDispatch</code> hooks to connect components to the Redux store:</p> <pre><code>import { useSelector, useDispatch } from \"react-redux\";\nimport { RootState, AppDispatch } from \"../app/store\";\nimport { setImages, nextImage } from \"../features/imageSlice\";\nimport { showLoader, hideLoader } from \"../features/loaderSlice\";\n\nconst ImageViewer = () =&gt; {\n  const dispatch = useDispatch&lt;AppDispatch&gt;();\n  const { images, currentViewIndex } = useSelector(\n    (state: RootState) =&gt; state.images\n  );\n  const { loading, message } = useSelector((state: RootState) =&gt; state.loader);\n\n  const handleNextImage = () =&gt; {\n    dispatch(nextImage());\n  };\n\n  // Component logic...\n};\n</code></pre>"},{"location":"frontend/state-management/#typed-hooks","title":"Typed Hooks","text":"<p>For better TypeScript support, we use typed versions of the hooks:</p> <pre><code>import { useDispatch, useSelector, TypedUseSelectorHook } from \"react-redux\";\nimport type { RootState, AppDispatch } from \"../app/store\";\n\nexport const useAppDispatch = () =&gt; useDispatch&lt;AppDispatch&gt;();\nexport const useAppSelector: TypedUseSelectorHook&lt;RootState&gt; = useSelector;\n</code></pre>"},{"location":"frontend/state-management/#best-practices","title":"Best Practices","text":"<ol> <li>Keep slices focused - Each slice should manage a specific domain of your application</li> <li>Normalize state shape - Use normalized data structures for complex relational data</li> <li>Use selectors - Create reusable selectors for complex state derivations (see <code>folderSelectors.ts</code>, <code>imageSelectors.ts</code>, <code>onboardingSelectors.ts</code>)</li> </ol>"},{"location":"frontend/state-management/#selectors","title":"Selectors","text":"<p>The application uses dedicated selector files for complex state derivations:</p> <ul> <li><code>folderSelectors.ts</code> - Folder-related selectors</li> <li><code>imageSelectors.ts</code> - Image-related selectors</li> <li><code>onboardingSelectors.ts</code> - Onboarding state selectors</li> </ul> <p>Example selector usage:</p> <pre><code>import { getFolderById } from \"@/features/folderSelectors\";\n\nconst folder = useSelector((state: RootState) =&gt;\n  getFolderById(state, folderId)\n);\n</code></pre> <p>This Redux-based architecture provides a scalable and maintainable state management solution that grows with our application's complexity.</p>"},{"location":"overview/architecture/","title":"Architecture","text":""},{"location":"overview/architecture/#frontend","title":"Frontend","text":"<p>For the frontend of our application, we use Tauri in combination with React. This allows us to create a desktop application with a web-based user interface. React handles the UI components and user interactions, while Tauri provides the bridge between our web-based frontend and Rust-based backend.</p> <p>Key points:</p> <ul> <li>Tauri: Enables building the desktop application</li> <li>React: Used for creating the user interface</li> <li>Rust: Powers the backend, which the frontend communicates with through Tauri's API</li> </ul> <p>This combination allows us to leverage web technologies for the UI while benefiting from Rust's performance and security for core functionalities.</p>"},{"location":"overview/architecture/#backend-python","title":"Backend Python","text":"<p> For the backend, we rely on several techstack, our database is served on sqlite while we using parallel processing capabilities of asyncio due to its compatibility with FastAPI.  Our models are from various sources, we use YOLO models for object and face detection while we use FaceNet for generating the embeddings of the faces detected. All these models are run on ONNX runtime to avoid heavy dependancies, keeping the application light weight.</p> <p>We use DBSCAN algorithm to perform clustering for face embeddings generated. All of our database is in SQL (sqlite) and our API calls rely on queries from the backend.</p> <p>Note</p> <p>We discuss all of the features and configuration of our application in further sections of the documentation. They can be used for both developers as well as users who want to use the app. A postman collection has also been added which can be found in our API section.  </p>"},{"location":"overview/architecture/#backend-rust-via-tauri","title":"Backend rust (via Tauri)","text":"<p>The Rust backend, integrated through Tauri, is a core component of our application. It leverages Rust's performance and safety features to handle file system operations, provide a secure bridge between the frontend and the local system, and manage OS-level interactions. This backend efficiently manages tasks such as reading and writing image files, extracting metadata, and ensuring secure access to system resources. It communicates with the React frontend through an IPC mechanism, allowing for seamless integration of low-level functionalities with the user interface. This architecture enables high-performance, secure operations on the local system while maintaining a smooth user experience.</p>"},{"location":"overview/features/","title":"PictoPy Features","text":""},{"location":"overview/features/#gallery-application","title":"Gallery Application","text":"<ul> <li>Intelligent Photo Tagging: Automatically tags photos based on detected objects, faces, and facial recognition.</li> <li>Traditional Gallery Management: Complete album organization and management tools.</li> </ul>"},{"location":"overview/features/#advanced-image-analysis","title":"Advanced Image Analysis","text":"<ul> <li>Object detection using YOLOv11 for identifying various items in images</li> <li>Face detection and clustering powered by FaceNet.</li> </ul>"},{"location":"overview/features/#privacy-focused-design","title":"Privacy-Focused Design","text":"<ul> <li>Entirely offline: All data stays on your local machine.</li> <li>No reliance on remote servers for processing.</li> <li>Models are stored locally and customizable by the user.</li> </ul>"},{"location":"overview/features/#efficient-data-handling-processing","title":"Efficient Data Handling &amp; Processing","text":"<ul> <li>Lightweight SQLite database for storing image metadata, face embeddings, and album info.</li> <li>Background image processing using <code>asyncio</code> for a smooth UI experience.</li> </ul>"},{"location":"overview/features/#smart-search-retrieval","title":"Smart Search &amp; Retrieval","text":"<ul> <li>Search photos based on:</li> <li>Detected objects</li> <li>Recognized faces</li> <li>Embedded metadata</li> <li>Find visually or semantically similar images</li> </ul>"},{"location":"overview/features/#cross-platform-compatibility","title":"Cross-Platform Compatibility","text":"<ul> <li>Available on major operating systems (Windows, macOS, Linux)</li> </ul>"},{"location":"overview/features/#technical-stack","title":"Technical Stack","text":"Component Technology Frontend React Desktop Framework Tauri Rust Backend Rust Python Backend Python Database SQLite Image Processing OpenCV, ONNX Runtime Object Detection YOLOv11 Face Recognition FaceNet API Framework FastAPI State Management Redux Toolkit Styling Tailwind CSS Routing React Router UI Components ShadCN Build Tool Vite Type Checking TypeScript"}]}